# Measuring Success
Measuring project success and learning from implementation is critical to long-term organizational and business viability.

**Note: what if the focus is how to do success monitoring on a shoestring?**

Goals of workforce and reducing fire risk or health is really the responsibility of gov or broader agencies to measure
But triple bottom line biz and org's want to measure social, economic, env impact
storytelling with data analysis

## Takeaways
- **Measure more than acres & bucks**. 
- **Effective success measurement is challenging**. Funding, implementing, and getting staff, stakeholders, and key audiences about M&E is difficult at best and impossible. It's also easy to get wrong, get left behind doing it at the project's end (since everyone is chasing the next funding or project implementation), and mindfully reflect on lessons learned. Look for ways to make it cost-feasible and maximize outcomes measurement.  Use remote sensing or citizen science volunteers to reduce costs and determine other ways to measure your outcomes. Donors and investors will pay attention to your results and iterative learning.
- **Theory -> Monitoring -> Learning**. Link your assessment or theory of change to your monitoring and learning
- **Take time to learn**. Build learning into our organizational culture. Plan a retreat to analyze results and write about project learning that you publish in blogs, posts, and journals.
- **Do the big picture 1st**. There is a tendency to jump straight to metrics and indicators before developing a theory of change and programmatic goals and objectives. Monitoring plans should tier from the big picture, so go through the process to develop the program strategy first. However, once developed, don't be afraid to change items, such as false assumptions or critical pathways that don't reflect reality.
- **Partner with science-based organizations**. If your business or organization doesn't have staff capacity or funding for measuring success, partner with an organization working in your region that does.

## Background
Like the Korean and Vietnam Wars in your high school history class, monitoring and evaluation are often forgotten, neglected, or not linked to learning and new or revised project design. Reporting is often treated as a required task, like toiling in the salt mines for your sustenance. But neither should be that way. Many organizations are missing a trick of learning from both activities to maximize their impact, attracting new investors/donors, and marching towards success. 

Unfortunately, not many donors cover monitoring costs, or monitoring is completed, but project managers and teams do not have time to analyze the data and create a learning environment around the results. Many resources are available to plan projects and develop monitoring plans. This guide is not an exhaustive resource but is focused on developing monitoring and measuring success when evaluation resources and capacity are limited.

FBA created the ProFrame/DIP worksheet to identify CAL FIRE metrics and set associated targets required for the application. This worksheet does not replace a good monitoring, evaluation, and learning (MEL) plan. Design teams, especially those creating complex, multi-year projects, should consider drafting a MEL plan for their own use that also responds to CAL FIRE’s information needs. Pages 38 through 43 of the Open Standards for the Practice of Conservation provide a good primer to help you create a MEL plan {cite}`cmp`. In developing a MEL plan, you would create specific, measurable, achievable, relevant, and time‑bound (SMART) indicators for all your goals, objectives, intermediate results, and outputs. The e-Civis Grant Portal Goals worksheet you will use to input project metrics and targets does not require fully articulated indicators. Nevertheless, developing such indicators and the rest of the MEL plan is worthwhile to facilitate your monitoring and evaluation work.

Monitoring could possibly be the subject written about the most but implemented the least in the nonprofit world. Why is this? Some thoughts:

1. Monitoring is hard to do
2. Monitoring is hard to do right
3. No staff capacity or applied scientists
4. Project funding doesn't including monitoring
5. It could demonstrate that your theory of change or organizational mission is incorrect. That's a big elephant in the room



{cite}`recipes`

## Monitoring plan
This is a logical point to start developing a monitoring plan. Please see the Measuring Success chapter for more details on developing this. Very briefly, however, a monitoring plan measures your progress on project goals and objectives by developing indicators or metrics linked to objectives. The plan creates a framework for the methodology, frequency, timing, and responsibility of measuring outcomes ({numref}`mel`). As shown in the figure, some plans include an at-a-glance graphic for trends in the metric status to assist managers in prioritizing their time to project success.

```{figure} /figures/mel.png
:name: mel
:height: 300px
Monitoring plan showing the relationship between results chains metrics and plan.
```

The monitoring plan should also indicate clear steps toward data analysis, learning, and adaptation. The Measuring Success chapter covers these steps in more detail.

```{caution}
Create a realistic monitoring plan for your budget, program, or business. Most leave monitoring for last, neglect it or fail to analyze and learn from the results. See the monitoring chapter for a practitioner's approach to developing and implementing a monitoring and learning plan.
```

## Data
Mobile field data collection appps such as kobo toolbox and mergin maps

## Data analysis

## Learning & adaptation
Assumptions. ICDP example. Mounting evidence for thinning and Rx fire from Malcolm's prebunking articles and compilation

Learning/writing workshops

[Pause & Reflect Toolkit](https://biodiversitylinks.org/library/resources/pause-and-reflect-toolkit.pdf/view)


## Caveats
- **Consult experts (but not too much)**. Consult experts and have them review your monitoring plan, methodologies, analyses, and results. However, do not look to academics or graduate students to run or fully develop your monitoring and learning. On the other hand, developing a research agenda with academics and students to guide research to your work and create learning that is deeper than monitoring results is extremely worthwhile. Here's why: applied research and monitoring are unlike in many ways. Research can guide your methods and direct your learning, but it is about finding levels of significance (>90% confidence) that are typically not feasible for programmatic implementation (~ > 75% but depends on the project, and the confidence may be qualitative). Researchers want to publish, and so should NGOs, but not as often in peer-reviewed journals. You should experiment at small scales and then scale when experimentation works. 
- **Create and revisit your monitoring plan**. 

## Resources
- **[Recipes for Conservation](http://gg.gg/1anhaa)**. A summary of how to develop the conservation standards within or among organizations. The recipes have some valuable sections for businesses but are nonprofit and agency-focused.

