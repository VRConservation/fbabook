# Measuring Success
Measuring project success and learning from implementation is critical to long-term organizational and business viability. Yet monitoring, analyzing, and communicating project outcomes and success is rarely prioritized by organizations and infrequently funded by donors. Given these realities, how do you implement measurement, analysis, and sharing results on a capacity and funding shoestring? We walk you through a quick-step guide to get started.

## Takeaways
- **Effective success measurement is challenging**. Funding, implementing, and getting staff, stakeholders, and key audiences about M&E is difficult at best and impossible. It's also easy to get wrong, get left behind doing it at the project's end (since everyone is chasing the next funding or project implementation), and mindfully reflect on lessons learned. Look for ways to make it cost-feasible and maximize outcomes measurement.  Use remote sensing or citizen science volunteers to reduce costs and determine other ways to measure your outcomes. Donors and investors will pay attention to your results and iterative learning.
- **Theory -> Monitoring -> Learning**. Link your assessment or theory of change to your monitoring and learning
- **Take time to learn**. Build learning into our organizational culture. Plan a retreat to analyze results and write about project learning that you publish in blogs, posts, and journals.
- **Do the big picture 1st**. There is a tendency to jump straight to metrics and indicators before developing a theory of change and programmatic goals and objectives. Monitoring plans should consider the big picture, so go through the process of developing the program strategy first. However, once developed, don't be afraid to change items, such as false assumptions or critical pathways that don't reflect reality.
- **Partner with science-based organizations**. If your business or organization doesn't have staff capacity or funding for measuring success, partner with an organization working in your region that does.
- **Triple-bottom line metrics**. biz and org's want to measure social, economic, env impact.
- **Storytelling with impact**. Writing workshops
- **Measure more than acres & bucks**. 


## Background
Monitoring and evaluation (M&E) are essential components of any successful project or program because they provide valuable insights into progress and effectiveness which organizations use to:

- ðŸŽ¯ Ensure accountability and demonstrate results to stakeholders;
- ðŸŽ¯ Inform decision making for improved program management and quality; and
- ðŸŽ¯ Provide information needed to generate revenue (fundraising, marketing, loans, etc.).

While there is no doubt that M&E is necessary, often organizations, especially nonprofits and small businesses, do not adequately implement M&E because of resource constraints â€“ lack of personnel, time, and budget. This document outlines five strategies for implementing M&E with limited resources.


Like the Korean and Vietnam Wars in your high school history class, monitoring and evaluation are often forgotten or neglected. Frequently, monitoring is not linked to learning and new or revised project design. Reporting is often treated as a required task, like toiling in the salt mines for your sustenance. But neither should be that way. Many organizations are missing a trick of learning from both activities to maximize their impact, attracting new investors/donors, and marching towards success. 

Unfortunately, not many donors cover monitoring costs, or monitoring is completed, but project managers and teams do not have time to analyze the data and create a learning environment around the results. Many resources are available to plan projects and develop monitoring plans. This guide is not an exhaustive resource but is focused on developing monitoring and measuring success when evaluation resources and capacity are limited.

FBA created the ProFrame/DIP worksheet to identify CAL FIRE metrics and set associated targets required for the application. This worksheet does not replace a good monitoring, evaluation, and learning (MEL) plan. Design teams, especially those creating complex, multi-year projects, should consider drafting a MEL plan for their own use that also responds to CAL FIREâ€™s information needs. Pages 38 through 43 of the Open Standards for the Practice of Conservation provide a good primer to help you create a MEL plan {cite}`cmp`. In developing a MEL plan, you would create specific, measurable, achievable, relevant, and timeâ€‘bound (SMART) indicators for all your goals, objectives, intermediate results, and outputs. The e-Civis Grant Portal Goals worksheet you will use to input project metrics and targets does not require fully articulated indicators. Nevertheless, developing such indicators and the rest of the MEL plan is worthwhile to facilitate your monitoring and evaluation work.

## Challenges
Monitoring could possibly be the subject written about the most but implemented the least in the nonprofit world. Why is this? Some thoughts:

1. Monitoring is hard to do
2. Monitoring is hard to do right
3. No staff capacity or applied scientists
4. Project funding doesn't including monitoring
5. It could demonstrate that your theory of change or organizational mission is incorrect. That's a big elephant in the room

{cite}`recipes`

## Monitoring plan
This is a logical point to start developing a monitoring plan. Please see the Measuring Success chapter for more details on developing this. Very briefly, however, a monitoring plan measures your progress on project goals and objectives by developing indicators or metrics linked to objectives. The plan creates a framework for the methodology, frequency, timing, and responsibility of measuring outcomes ({numref}`mel`). As shown in the figure, some plans include an at-a-glance graphic for trends in the metric status to assist managers in prioritizing their time to project success.

```{figure} /figures/mel.png
:name: mel
:height: 300px
Monitoring plan showing the relationship between results chains metrics and plan.
```

The monitoring plan should also indicate clear steps toward data analysis, learning, and adaptation. The Measuring Success chapter covers these steps in more detail.

```{caution}
Create a realistic monitoring plan for your budget, program, or business. Most leave monitoring for last, neglect it or fail to analyze and learn from the results. See the monitoring chapter for a practitioner's approach to developing and implementing a monitoring and learning plan.
```

## Data
Mobile field data collection appps such as kobo toolbox and mergin maps

## Data analysis

## Learning & adaptation
Assumptions. ICDP example. Mounting evidence for thinning and Rx fire from Malcolm's prebunking articles and compilation

Learning/writing workshops

[Pause & Reflect Toolkit](https://biodiversitylinks.org/library/resources/pause-and-reflect-toolkit.pdf/view)

## Yugo vs. Cadillac
What is the absolute bare bones monitoring system that could be implemented (Yugo) vs. a higher end deal (Cadillac)? We suggest a couple of examples in the drop downs.

```{admonition} Yugo Monitoring Plan
:class: dropdown


```

```{note}
:class: dropdown

This admonition has been collapsed,
meaning you can add longer form content here,
```

```{admonition} My custom title with *Markdown*!
:class: dropdown

This is a custom title for a tip admonition.
```

## Resources
- **[Recipes for Conservation](http://gg.gg/1anhaa)**. A summary of how to develop the conservation standards within or among organizations. The recipes have some valuable sections for businesses but are nonprofit and agency-focused.