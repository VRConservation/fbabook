# Measuring Success
Measuring project success and learning from implementation is critical to long-term organizational and business viability.

**Note: what if the focus is how to do success monitoring on a shoestring?**

## Takeaways
- **Effective success measurement is challenging**. Funding, implementing, and getting staff, stakeholders, and key audiences about 'M&E' is difficult at best and impossible. It's also easy to get wrong, get left behind doing it at the project's end (since everyone is chasing the next funding or project implementation), and mindfully reflect on lessons learned. Look for ways to make it cost-feasible and maximize outcomes measurement.  Use remote sensing or citizen science volunteers to reduce costs and determine other ways to measure your outcomes. Donors and investors will pay attention to your results and iterative learning.
- **Theory -> Monitoring -> Learning**. Link your assessment or theory of change to your monitoring and learning
- **Take time to learn**. Build learning into our organizational culture. Plan a retreat to analyze results and write about project learning that you publish in blogs, posts, and journals.
- **Do the big picture 1st**. There is a tendency to jump straight to metrics and indicators before developing a theory of change and programmatic goals and objectives. Monitoring plans should tier from the big picture, so go through the process to develop the program strategy first. However, once developed, don't be afraid to change items, such as false assumptions or critical pathways that don't reflect reality.

## Background
Like the Korean and Vietnam Wars in your high school history class, monitoring and evaluation are often forgotten, neglected, or not linked to learning and new or revised project design. Reporting is often treated as a required task, like toiling in the salt mines for your sustenance. But neither should be that way. Many organizations are missing a trick of learning from both activities to maximize their impact, attracting new investors/donors, and marching towards success. 

Unfortunately, not many donors cover monitoring costs, or monitoring is completed, but project managers and teams do not have time to analyze the data and create a learning environment around the results. Many resources are available to plan projects and develop monitoring plans. This guide is not an exhaustive resource but is focused on developing monitoring and measuring success when evaluation resources and capacity are limited.

FBA created the ProFrame/DIP worksheet to identify CAL FIRE metrics and set associated targets required for the application. This worksheet does not replace a good monitoring, evaluation, and learning (MEL) plan. Design teams, especially those creating complex, multi-year projects, should consider drafting a MEL plan for their own use that also responds to CAL FIRE’s information needs. Pages 38 through 43 of the Open Standards for the Practice of Conservation provide a good primer to help you create a MEL plan {cite}`cmp`. In developing a MEL plan, you would create specific, measurable, achievable, relevant, and time‑bound (SMART) indicators for all your goals, objectives, intermediate results, and outputs. The e-Civis Grant Portal Goals worksheet you will use to input project metrics and targets does not require fully articulated indicators. Nevertheless, developing such indicators and the rest of the MEL plan is worthwhile to facilitate your monitoring and evaluation work.

{cite}`recipes`

## Monitoring plan

## Data

## Data analysis

## Learning & adaptation
Assumptions. ICDP example. Mounting evidence for thinning and Rx fire from Malcolm's prebunking articles and compilation

Learning/writing workshops

[Pause & Reflect Toolkit](https://biodiversitylinks.org/library/resources/pause-and-reflect-toolkit.pdf/view)


## Caveats
- **Consult experts (but not too much)**. Consult experts and have them review your monitoring plan, methodologies, analyses, and results. However, do not look to academics or graduate students to run or fully develop your monitoring and learning. On the other hand, developing a research agenda with academics and students to guide research to your work and create learning that is deeper than monitoring results is extremely worthwhile. Here's why: applied research and monitoring are unlike in many ways. Research can guide your methods and direct your learning, but it is about finding levels of significance (>90% confidence) that are typically not feasible for programmatic implementation (~ > 75% but depends on the project, and the confidence may be qualitative). Researchers want to publish, and so should NGOs, but not as often in peer-reviewed journals. You should experiment at small scales and then scale when experimentation works. 
- **Create and revisit your monitoring plan**. 

## Resources
- **[Recipes for Conservation](http://gg.gg/1anhaa)**. A summary of how to develop the conservation standards within or among organizations. The recipes have some valuable sections for businesses but are nonprofit and agency-focused.

