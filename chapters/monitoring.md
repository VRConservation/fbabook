# Measuring Success
Measuring project success and learning from implementation is critical to long-term organizational and business viability. If you do it right it will make your smarter, learn faster, and increase investment in your work. Ignore it at your peril!

## Takeaways
- **Effective success measurement is challenging**. Funding, implementing, and getting staff, stakeholders, and key audiences about 'M&E' is difficult at best, impossible at worst. It's also easy to get wrong, get left behind doing it at project's end (since everyone is chasing the next funding or project implementation), and mindfully reflect on lessons learned. Look for ways to make it cost feasible and maximize outcomes measurement.  Use remote sensing or citizen science volunteers to reduce costs and determine other ways to measure your outcomes. Donors and investors will pay attention to your results and iterative learning.
- **Theory -> Monitoring -> Learning**. Link your assessment or theory of change to your monitoring and learning
- **Take time to learn**. Build learning into our organizational culture. Plan a retreat to analyze results and write about project learning that you publish in blogs, posts, and journals.

## Background
Like the Korean and Vietnam Wars in your high school history class, monitoring and evaluation is often forgotten, neglected, or not linked to learning and new project design. Reporting is often treated as a required task like toiling in the salt mines for your sustenance. But neither should be that way and many organizations are missing a trick of learning from both of these activities. 

Unfortunately not many funders cover monitoring costs or monitoring is completed but project managers and teams do not have time to analyze the data and create a learning environment around the results.

FBA has included worksheets to help you track progress and prepare reports if CAL FIRE funds your project. You do not need to complete these worksheets during the design phase.

{cite}`recipes`

## Monitoring plan

## Data

## Data analysis

## Learning & adaptation
Assumptions. ICDP example. Mounting evidence for thinning and Rx fire from Malcolm's prebunking articles and compilation

Learning/writing workshops

[Pause & Reflect Toolkit](https://biodiversitylinks.org/library/resources/pause-and-reflect-toolkit.pdf/view)


## Caveats
- **Consult experts (but not too much)**. Consult experts and have them review your monitoring plan, methodologies, analyses, and results. However, do not look to academices or graduate students to run or fully develop your monitoring and learning. On the other hand, it is extremely wortwhile to develop a research agenda with academics and students to guide research to your work and create learning that is deeper than monitoring results. Here's why: applied research and monitoring are unlike in many ways. Research can guide your methods and direct your learning but it is about finding levels of significance (>90% confidence) that are typically not feasible for programmatic implementation (~ > 75% but depends on the project and the confidence may be qualitative). Researchers want to publish, and so should NGO's but not as often in peer reviewed journals. You should experiement at small scales then scale when experimentation works. 
- **Create and revisit your monitoring plan**. 

## Resources
- **[Recipes for Conservation](http://gg.gg/1anhaa)**. A summary of how to develop the conservation standards within or among organizations. The recipes have some valuable sections for businesses, but are nonprofit and agency focused.